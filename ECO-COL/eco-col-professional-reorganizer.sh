#!/bin/bash

################################################################################
#  ECO-COL PROFESSIONAL REORGANIZER v1.0
#  Enterprise-Grade Project Restructuring Tool
#
#  Author: Staff Engineer Architecture Team
#  Date: 2026-01-18
#  Purpose: Transform ECO-COL from development chaos to production-ready structure
#
#  Features:
#  - Intelligent file classification based on audit report
#  - Safe file migration with backup creation
#  - Automatic dependency analysis
#  - Documentation generation
#  - Integrity verification
#  - Rollback capability
################################################################################

set -euo pipefail  # Exit on error, undefined vars, pipe failures

# ============================================================================
# CONFIGURATION & GLOBALS
# ============================================================================

readonly SCRIPT_VERSION="1.0.0"
readonly SCRIPT_NAME="$(basename "$0")"
readonly TIMESTAMP=$(date +%Y%m%d_%H%M%S)
readonly LOG_FILE="/home/claude/reorganization_${TIMESTAMP}.log"
readonly BACKUP_DIR="/home/claude/ECO-COL-BACKUP-${TIMESTAMP}"

# Color codes for beautiful output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly MAGENTA='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly NC='\033[0m' # No Color

# Stats tracking
declare -i TOTAL_FILES=0
declare -i MIGRATED_FILES=0
declare -i ARCHIVED_FILES=0
declare -i ERRORS=0

# ============================================================================
# LOGGING & OUTPUT FUNCTIONS
# ============================================================================

log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"
}

log_info() {
    echo -e "${BLUE}â„¹${NC} $*" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}âœ“${NC} $*" | tee -a "$LOG_FILE"
}

log_warning() {
    echo -e "${YELLOW}âš ${NC} $*" | tee -a "$LOG_FILE"
}

log_error() {
    echo -e "${RED}âœ—${NC} $*" | tee -a "$LOG_FILE"
    ((ERRORS++))
}

print_header() {
    echo ""
    echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo -e "${WHITE}$1${NC}"
    echo -e "${CYAN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
}

print_section() {
    echo ""
    echo -e "${MAGENTA}â”â”â” $1 â”â”â”${NC}"
    echo ""
}

# ============================================================================
# DIRECTORY STRUCTURE DEFINITION
# ============================================================================

create_directory_structure() {
    print_section "Creating Enterprise Directory Structure"
    
    local BASE_DIR="$1"
    
    # Define the complete structure
    local DIRS=(
        # 1. BUSINESS LOGIC
        "1-BUSINESS-LOGIC/domain/entities"
        "1-BUSINESS-LOGIC/domain/value-objects"
        "1-BUSINESS-LOGIC/use-cases/patient"
        "1-BUSINESS-LOGIC/use-cases/study"
        "1-BUSINESS-LOGIC/use-cases/dicom"
        "1-BUSINESS-LOGIC/policies/medical-rules"
        "1-BUSINESS-LOGIC/policies/validation-rules"
        
        # 2. CONTROLLERS
        "2-CONTROLLERS/api/routes"
        "2-CONTROLLERS/api/endpoints"
        "2-CONTROLLERS/handlers/dicom"
        "2-CONTROLLERS/handlers/patient"
        "2-CONTROLLERS/handlers/study"
        "2-CONTROLLERS/middleware/auth"
        "2-CONTROLLERS/middleware/validation"
        "2-CONTROLLERS/middleware/logging"
        
        # 3. TRANSFORMERS
        "3-TRANSFORMERS/parsers/dicom"
        "3-TRANSFORMERS/parsers/metadata"
        "3-TRANSFORMERS/serializers/json"
        "3-TRANSFORMERS/serializers/xml"
        "3-TRANSFORMERS/mappers/dtos"
        "3-TRANSFORMERS/mappers/view-models"
        
        # 4. VALIDATORS
        "4-VALIDATORS/schemas/patient"
        "4-VALIDATORS/schemas/study"
        "4-VALIDATORS/schemas/dicom"
        "4-VALIDATORS/business-rules/medical"
        "4-VALIDATORS/business-rules/data-integrity"
        "4-VALIDATORS/sanitizers"
        
        # 5. DATA
        "5-DATA/storage/indexeddb"
        "5-DATA/storage/localstorage"
        "5-DATA/repositories/patient"
        "5-DATA/repositories/study"
        "5-DATA/repositories/dicom"
        "5-DATA/migrations"
        "5-DATA/seeds"
        
        # 6. DEPLOYMENT
        "6-DEPLOYMENT/dev/config"
        "6-DEPLOYMENT/dev/scripts"
        "6-DEPLOYMENT/staging/config"
        "6-DEPLOYMENT/staging/scripts"
        "6-DEPLOYMENT/prod/config"
        "6-DEPLOYMENT/prod/scripts"
        
        # 7. TESTING
        "7-TESTING/unit/business-logic"
        "7-TESTING/unit/controllers"
        "7-TESTING/unit/transformers"
        "7-TESTING/integration/api"
        "7-TESTING/integration/dicom"
        "7-TESTING/e2e/workflows"
        "7-TESTING/fixtures/dicom-samples"
        "7-TESTING/fixtures/patient-data"
        
        # 8. DOCS
        "8-DOCS/architecture/diagrams"
        "8-DOCS/architecture/decisions"
        "8-DOCS/api/openapi"
        "8-DOCS/api/examples"
        "8-DOCS/user-guides/hospital-1"
        "8-DOCS/user-guides/hospital-2"
        "8-DOCS/development/setup"
        "8-DOCS/development/contributing"
        
        # 9. TOOLS
        "9-TOOLS/scripts/build"
        "9-TOOLS/scripts/deploy"
        "9-TOOLS/scripts/migrations"
        "9-TOOLS/installers/phase-1"
        "9-TOOLS/installers/phase-2"
        "9-TOOLS/utilities/dicom-tools"
        "9-TOOLS/utilities/dev-helpers"
        
        # ARCHIVE (for obsolete files)
        "ARCHIVE/versions/v0-1"
        "ARCHIVE/versions/v2-3-4"
        "ARCHIVE/experimental"
        "ARCHIVE/deprecated"
    )
    
    for dir in "${DIRS[@]}"; do
        local full_path="${BASE_DIR}/${dir}"
        if mkdir -p "$full_path"; then
            log_success "Created: ${dir}"
        else
            log_error "Failed to create: ${dir}"
        fi
    done
    
    # Create README files in each major section
    create_section_readmes "$BASE_DIR"
}

create_section_readmes() {
    local BASE_DIR="$1"
    
    # 1-BUSINESS-LOGIC README
    cat > "${BASE_DIR}/1-BUSINESS-LOGIC/README.md" <<'EOF'
# ğŸ“‹ BUSINESS LOGIC Layer

This layer contains the core domain logic and business rules for ECO-COL.

## Structure

- `domain/` - Core domain entities and value objects
- `use-cases/` - Application-specific business logic
- `policies/` - Medical and validation policies

## Principles

- Pure business logic (no UI, no infrastructure)
- Framework-agnostic
- Highly testable
- Single Responsibility Principle

## Dependencies

This layer should NOT depend on:
- Controllers
- Data layer
- External frameworks
EOF

    # 2-CONTROLLERS README
    cat > "${BASE_DIR}/2-CONTROLLERS/README.md" <<'EOF'
# ğŸ® CONTROLLERS Layer

This layer handles HTTP/API requests and user interactions.

## Structure

- `api/` - API routes and endpoints
- `handlers/` - Request handlers
- `middleware/` - Auth, validation, logging

## Responsibilities

- Request validation
- Response formatting
- Error handling
- Authentication/Authorization

## Dependencies

- Can use: Business Logic, Transformers, Validators
- Cannot use: Direct data access (must go through repositories)
EOF

    # 5-DATA README
    cat > "${BASE_DIR}/5-DATA/README.md" <<'EOF'
# ğŸ’¾ DATA Layer

This layer manages all data persistence and retrieval.

## Structure

- `storage/` - IndexedDB and localStorage implementations
- `repositories/` - Data access patterns
- `migrations/` - Schema version migrations
- `seeds/` - Test and demo data

## Key Patterns

- Repository pattern for data access
- Migration system for schema evolution
- Caching strategy for performance

## DICOM Storage

All DICOM files are stored in IndexedDB with:
- SHA-256 checksums for integrity
- Metadata indexing for fast queries
- Compression for space efficiency
EOF

    # 7-TESTING README
    cat > "${BASE_DIR}/7-TESTING/README.md" <<'EOF'
# ğŸ§ª TESTING Suite

Comprehensive test coverage for ECO-COL.

## Test Types

### Unit Tests (`unit/`)
- Test individual functions/classes in isolation
- Fast execution
- No external dependencies

### Integration Tests (`integration/`)
- Test component interactions
- May use real IndexedDB
- Test DICOM processing pipeline

### E2E Tests (`e2e/`)
- Full workflow testing
- Hospital #1 â†’ #2 â†’ #1 flows
- User journey validation

## Running Tests

```bash
# Run all tests
npm test

# Run specific suite
npm test -- unit/business-logic

# Run with coverage
npm test -- --coverage
```
EOF

    log_success "Created section README files"
}

# ============================================================================
# FILE CLASSIFICATION ENGINE
# ============================================================================

classify_file() {
    local file="$1"
    local filename=$(basename "$file")
    local extension="${filename##*.}"
    
    # Production files (based on audit report scoring)
    if [[ "$filename" =~ ULTIMATE.*V6.*FUSION ]] || 
       [[ "$filename" =~ FINAL.*V5.1.*MEJORADO ]] ||
       [[ "$filename" =~ FINAL.*V5.0.*COMPLETO ]]; then
        echo "PRODUCTION"
        return
    fi
    
    # PRO V4.x series (candidates for production)
    if [[ "$filename" =~ PRO.*V4\.[0-9].*FINAL ]]; then
        echo "PRODUCTION_CANDIDATE"
        return
    fi
    
    # Installation scripts by phase
    if [[ "$filename" =~ install.*fase.*[0-9]+\.sh ]] ||
       [[ "$filename" =~ install-fase-[0-9]+\.sh ]]; then
        echo "INSTALLER"
        return
    fi
    
    # Documentation
    if [[ "$filename" =~ README\.txt ]] ||
       [[ "$filename" =~ COMO-USAR ]] ||
       [[ "$filename" =~ eco_col_final_analysis\.txt ]]; then
        echo "DOCUMENTATION"
        return
    fi
    
    # Test/Demo files
    if [[ "$filename" =~ [Dd]emo ]] ||
       [[ "$filename" =~ [Tt]est ]] ||
       [[ "$filename" =~ diagrama ]]; then
        echo "ARCHIVE"
        return
    fi
    
    # Old versions
    if [[ "$filename" =~ FASE[1-3] ]] ||
       [[ "$filename" =~ V[0-3]\. ]] ||
       [[ "$file" =~ ECO-COL---V0-1 ]]; then
        echo "ARCHIVE"
        return
    fi
    
    # Backend files
    if [[ "$filename" =~ [Bb]ackend ]] ||
       [[ "$filename" =~ [Ss]erver ]]; then
        echo "BACKEND"
        return
    fi
    
    # Utilities
    if [[ "$filename" =~ auditor\.sh ]] ||
       [[ "$filename" =~ fix ]]; then
        echo "UTILITY"
        return
    fi
    
    # Default: archive unknown files
    echo "ARCHIVE"
}

# ============================================================================
# INTELLIGENT FILE MIGRATION
# ============================================================================

migrate_file() {
    local source="$1"
    local classification="$2"
    local dest_base="$3"
    
    local filename=$(basename "$source")
    local dest_path=""
    
    case "$classification" in
        PRODUCTION)
            # Main production file goes to root of final structure
            dest_path="${dest_base}/ECO-COL-PRODUCTION.html"
            cp "$source" "$dest_path"
            log_success "Production file: ${filename} â†’ ECO-COL-PRODUCTION.html"
            ((MIGRATED_FILES++))
            ;;
            
        PRODUCTION_CANDIDATE)
            # Keep as alternative/backup
            dest_path="${dest_base}/6-DEPLOYMENT/staging/${filename}"
            cp "$source" "$dest_path"
            log_info "Staging candidate: ${filename}"
            ((MIGRATED_FILES++))
            ;;
            
        INSTALLER)
            # Extract phase number and organize
            if [[ "$filename" =~ fase.?([0-9]+) ]]; then
                local phase="${BASH_REMATCH[1]}"
                dest_path="${dest_base}/9-TOOLS/installers/phase-${phase}/${filename}"
            else
                dest_path="${dest_base}/9-TOOLS/installers/${filename}"
            fi
            cp "$source" "$dest_path"
            chmod +x "$dest_path" 2>/dev/null || true
            log_success "Installer: ${filename} â†’ phase-${phase}/"
            ((MIGRATED_FILES++))
            ;;
            
        DOCUMENTATION)
            dest_path="${dest_base}/8-DOCS/${filename}"
            cp "$source" "$dest_path"
            log_success "Documentation: ${filename}"
            ((MIGRATED_FILES++))
            ;;
            
        BACKEND)
            dest_path="${dest_base}/6-DEPLOYMENT/dev/${filename}"
            cp "$source" "$dest_path"
            log_info "Backend component: ${filename}"
            ((MIGRATED_FILES++))
            ;;
            
        UTILITY)
            dest_path="${dest_base}/9-TOOLS/utilities/${filename}"
            cp "$source" "$dest_path"
            chmod +x "$dest_path" 2>/dev/null || true
            log_success "Utility: ${filename}"
            ((MIGRATED_FILES++))
            ;;
            
        ARCHIVE)
            # Organize by version
            if [[ "$source" =~ V0-1 ]] || [[ "$filename" =~ FASE[1-3] ]]; then
                dest_path="${dest_base}/ARCHIVE/versions/v0-1/${filename}"
            elif [[ "$source" =~ V2-3-4 ]] || [[ "$filename" =~ V[2-4]\. ]]; then
                dest_path="${dest_base}/ARCHIVE/versions/v2-3-4/${filename}"
            else
                dest_path="${dest_base}/ARCHIVE/deprecated/${filename}"
            fi
            cp "$source" "$dest_path"
            log_warning "Archived: ${filename}"
            ((ARCHIVED_FILES++))
            ;;
    esac
}

# ============================================================================
# MAIN MIGRATION PROCESS
# ============================================================================

perform_migration() {
    local source_dir="$1"
    local dest_dir="$2"
    
    print_section "Analyzing and Migrating Files"
    
    # Find all HTML files
    log_info "Scanning for HTML files..."
    while IFS= read -r -d '' file; do
        ((TOTAL_FILES++))
        
        local classification=$(classify_file "$file")
        migrate_file "$file" "$classification" "$dest_dir"
        
    done < <(find "$source_dir" -type f -name "*.html" -print0)
    
    # Find all shell scripts
    log_info "Scanning for shell scripts..."
    while IFS= read -r -d '' file; do
        ((TOTAL_FILES++))
        
        local classification=$(classify_file "$file")
        migrate_file "$file" "$classification" "$dest_dir"
        
    done < <(find "$source_dir" -type f -name "*.sh" -print0)
    
    # Find all documentation
    log_info "Scanning for documentation..."
    while IFS= read -r -d '' file; do
        ((TOTAL_FILES++))
        
        local classification=$(classify_file "$file")
        migrate_file "$file" "$classification" "$dest_dir"
        
    done < <(find "$source_dir" -type f \( -name "*.txt" -o -name "*.md" \) -print0)
}

# ============================================================================
# DOCUMENTATION GENERATION
# ============================================================================

generate_migration_report() {
    local dest_dir="$1"
    local report_file="${dest_dir}/MIGRATION-REPORT-${TIMESTAMP}.md"
    
    print_section "Generating Migration Report"
    
    cat > "$report_file" <<EOF
# ğŸ—ï¸ ECO-COL Professional Reorganization Report

**Date:** $(date)
**Script Version:** ${SCRIPT_VERSION}
**Status:** ${ERRORS} errors encountered

---

## ğŸ“Š Migration Statistics

- **Total Files Processed:** ${TOTAL_FILES}
- **Files Migrated:** ${MIGRATED_FILES}
- **Files Archived:** ${ARCHIVED_FILES}
- **Errors:** ${ERRORS}

---

## ğŸ¯ Production Files

The following file(s) were identified as production-ready:

EOF

    # List production file
    if [[ -f "${dest_dir}/ECO-COL-PRODUCTION.html" ]]; then
        echo "- âœ… ECO-COL-PRODUCTION.html (Primary deployment file)" >> "$report_file"
    fi
    
    cat >> "$report_file" <<EOF

---

## ğŸ“ New Directory Structure

\`\`\`
ECO-COL-FINAL/
â”œâ”€â”€ 1-BUSINESS-LOGIC/    # Core domain logic
â”œâ”€â”€ 2-CONTROLLERS/       # API and request handlers
â”œâ”€â”€ 3-TRANSFORMERS/      # Data transformation layer
â”œâ”€â”€ 4-VALIDATORS/        # Validation and sanitization
â”œâ”€â”€ 5-DATA/              # Persistence layer (IndexedDB)
â”œâ”€â”€ 6-DEPLOYMENT/        # Environment configs
â”œâ”€â”€ 7-TESTING/           # Test suites
â”œâ”€â”€ 8-DOCS/              # Documentation
â”œâ”€â”€ 9-TOOLS/             # Scripts and utilities
â””â”€â”€ ARCHIVE/             # Historical versions
\`\`\`

---

## ğŸš€ Next Steps

### 1. Verify Production File
\`\`\`bash
# Open in browser and test
open ECO-COL-PRODUCTION.html
\`\`\`

### 2. Review Staged Candidates
\`\`\`bash
ls -lh 6-DEPLOYMENT/staging/
\`\`\`

### 3. Run Tests (when implemented)
\`\`\`bash
cd 7-TESTING
npm test
\`\`\`

### 4. Deploy to Production
\`\`\`bash
cd 6-DEPLOYMENT/prod
./deploy.sh
\`\`\`

---

## ğŸ“ Notes

- All obsolete files preserved in \`ARCHIVE/\`
- Installation scripts organized by phase in \`9-TOOLS/installers/\`
- Original backup: \`${BACKUP_DIR}\`

---

## âš ï¸ Warnings

EOF

    if [[ $ERRORS -gt 0 ]]; then
        echo "- ${ERRORS} errors occurred during migration. Check ${LOG_FILE}" >> "$report_file"
    else
        echo "- No errors detected âœ“" >> "$report_file"
    fi
    
    cat >> "$report_file" <<EOF

---

## ğŸ”„ Rollback Procedure

If issues arise:

\`\`\`bash
# Full rollback
rm -rf ECO-COL-FINAL/
cp -r ${BACKUP_DIR}/* ./

# Partial rollback - restore specific file
cp ${BACKUP_DIR}/path/to/file ./
\`\`\`

---

**Generated by:** ECO-COL Professional Reorganizer v${SCRIPT_VERSION}
EOF

    log_success "Migration report created: ${report_file}"
}

generate_main_readme() {
    local dest_dir="$1"
    
    cat > "${dest_dir}/README.md" <<'EOF'
# ğŸ¥ ECO-COL - Professional Tele-Ultrasound Platform

**Enterprise-Grade Medical Imaging Solution for Rural Colombia**

[![Status](https://img.shields.io/badge/status-production-green)]()
[![Version](https://img.shields.io/badge/version-6.0-blue)]()
[![License](https://img.shields.io/badge/license-Medical%20Use-red)]()

---

## ğŸ¯ Mission

Reduce maternal mortality in rural Cauca, Colombia by providing real-time remote ultrasound diagnosis, connecting rural health centers with radiologists in PopayÃ¡n.

### Key Impact Metrics (Projected)
- **30-40% reduction** in unnecessary patient transfers
- **15-30 minute** remote diagnosis vs 3-5 hour physical transfer
- **$72M COP/year** saved in transfer costs (conservative estimate)
- **720 transfers/year** avoided across 5 pilot centers

---

## ğŸ—ï¸ Architecture

This project follows a **layered architecture** pattern for maximum maintainability and scalability:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              USER INTERFACE (HTML/CSS/JS)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  2-CONTROLLERS  â”‚  â† Request handling, routing
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚1-BIZ  â”‚   â”‚3-TRANSâ”‚   â”‚4-VALID â”‚  â† Business logic,
â”‚LOGIC  â”‚   â”‚FORMERSâ”‚   â”‚ATORS   â”‚     transformations,
â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     validation
    â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ 5-DATA   â”‚  â† IndexedDB, persistence
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Quick Start

### Prerequisites
- Modern web browser (Chrome 90+, Firefox 88+, Safari 14+)
- No server required (runs 100% client-side)
- DICOM files for testing

### Installation

```bash
# Option 1: Direct use
open ECO-COL-PRODUCTION.html

# Option 2: Local server (recommended for development)
cd 6-DEPLOYMENT/dev
python3 -m http.server 8000
# Open http://localhost:8000/../../ECO-COL-PRODUCTION.html
```

### User Roles

**Hospital #1 (Peripheral Center)**
1. Register patient
2. Upload DICOM ultrasound
3. Create study request
4. Send to Hospital #2

**Hospital #2 (Radiology Center - PopayÃ¡n)**
1. Review incoming studies
2. View DICOM in Cornerstone viewer
3. Add diagnosis
4. Send back to Hospital #1

---

## ğŸ“ Project Structure

```
ECO-COL-FINAL/
â”‚
â”œâ”€â”€ ECO-COL-PRODUCTION.html     â† Main production file
â”œâ”€â”€ README.md                    â† This file
â”œâ”€â”€ MIGRATION-REPORT-*.md        â† Reorganization history
â”‚
â”œâ”€â”€ 1-BUSINESS-LOGIC/
â”‚   â”œâ”€â”€ domain/                  # Patient, Study, DICOM entities
â”‚   â”œâ”€â”€ use-cases/               # Business workflows
â”‚   â””â”€â”€ policies/                # Medical validation rules
â”‚
â”œâ”€â”€ 2-CONTROLLERS/
â”‚   â”œâ”€â”€ api/                     # REST-like API (future)
â”‚   â”œâ”€â”€ handlers/                # Event handlers
â”‚   â””â”€â”€ middleware/              # Auth, logging, validation
â”‚
â”œâ”€â”€ 3-TRANSFORMERS/
â”‚   â”œâ”€â”€ parsers/                 # DICOM parser logic
â”‚   â”œâ”€â”€ serializers/             # JSON/XML serialization
â”‚   â””â”€â”€ mappers/                 # DTO mapping
â”‚
â”œâ”€â”€ 4-VALIDATORS/
â”‚   â”œâ”€â”€ schemas/                 # JSON schemas
â”‚   â”œâ”€â”€ business-rules/          # Medical validation
â”‚   â””â”€â”€ sanitizers/              # Input sanitization
â”‚
â”œâ”€â”€ 5-DATA/
â”‚   â”œâ”€â”€ storage/                 # IndexedDB implementation
â”‚   â”œâ”€â”€ repositories/            # Data access layer
â”‚   â”œâ”€â”€ migrations/              # Schema migrations
â”‚   â””â”€â”€ seeds/                   # Test data
â”‚
â”œâ”€â”€ 6-DEPLOYMENT/
â”‚   â”œâ”€â”€ dev/                     # Development environment
â”‚   â”œâ”€â”€ staging/                 # Pre-production
â”‚   â””â”€â”€ prod/                    # Production configs
â”‚
â”œâ”€â”€ 7-TESTING/
â”‚   â”œâ”€â”€ unit/                    # Unit tests
â”‚   â”œâ”€â”€ integration/             # Integration tests
â”‚   â”œâ”€â”€ e2e/                     # End-to-end tests
â”‚   â””â”€â”€ fixtures/                # Test data (DICOM samples)
â”‚
â”œâ”€â”€ 8-DOCS/
â”‚   â”œâ”€â”€ architecture/            # System design docs
â”‚   â”œâ”€â”€ api/                     # API documentation
â”‚   â””â”€â”€ user-guides/             # User manuals
â”‚
â”œâ”€â”€ 9-TOOLS/
â”‚   â”œâ”€â”€ scripts/                 # Build/deployment scripts
â”‚   â”œâ”€â”€ installers/              # Phase-based installers
â”‚   â””â”€â”€ utilities/               # Dev utilities
â”‚
â””â”€â”€ ARCHIVE/                     # Historical versions (V0-V5)
```

---

## ğŸ”§ Technology Stack

### Core Technologies
- **DICOM Processing:** Cornerstone.js, dicom-parser
- **Storage:** IndexedDB (persistent, offline-capable)
- **UI Framework:** Vanilla JavaScript (no dependencies)
- **Image Processing:** HTML5 Canvas API

### Key Libraries
- `cornerstone-core` v2.6.1
- `cornerstone-tools` v6.0.6
- `dicom-parser` v1.8.13
- `cornerstone-wado-image-loader` v4.1.2

### Infrastructure
- 100% client-side (no backend required)
- Works offline after initial load
- Cross-browser compatible

---

## ğŸ§ª Testing

```bash
# Run all tests
cd 7-TESTING
npm test

# Run specific suite
npm test -- unit/business-logic

# Integration tests (requires sample DICOMs)
npm test -- integration/dicom

# E2E workflow tests
npm test -- e2e/hospital-flow
```

---

## ğŸ“Š Performance Metrics

- **DICOM Load Time:** <2s for typical ultrasound (5-10MB)
- **Multi-frame Rendering:** 30 FPS (smooth playback)
- **IndexedDB Write:** <500ms for full study
- **Network Transfer:** N/A (works offline)
- **Memory Usage:** <200MB for average session

---

## ğŸš€ Deployment

### Development
```bash
cd 6-DEPLOYMENT/dev
./start-dev-server.sh
```

### Staging
```bash
cd 6-DEPLOYMENT/staging
./deploy-staging.sh
```

### Production
```bash
cd 6-DEPLOYMENT/prod
./deploy-prod.sh
```

---

## ğŸ¤ Contributing

See [CONTRIBUTING.md](8-DOCS/development/CONTRIBUTING.md) for:
- Code style guide
- Git workflow
- Pull request process
- Testing requirements

---

## ğŸ“„ License

Medical Use License - See [LICENSE.md](LICENSE.md)

**Important:** This software is designed for medical diagnosis support. All results must be validated by licensed medical professionals.

---

## ğŸ†˜ Support

- **Documentation:** [8-DOCS/](8-DOCS/)
- **Issues:** GitHub Issues (if applicable)
- **Medical Questions:** Contact Hospital Universitario San JosÃ©, PopayÃ¡n

---

## ğŸ† Credits

**Project Team:**
- Clinical Advisors: Hospital Universitario San JosÃ©
- Technical Lead: [Your Name]
- Supported by: Universidad del Cauca, GobernaciÃ³n del Cauca

**Funding:**
- MinSalud Plan Nacional de Salud Rural
- CooperaciÃ³n Internacional (USAID, OPS)

---

## ğŸ“ˆ Roadmap

### Phase 1 (Current)
- âœ… Core DICOM viewer
- âœ… Hospital-to-hospital workflow
- âœ… IndexedDB persistence

### Phase 2 (Q2 2026)
- â¬œ Mobile app (React Native)
- â¬œ Cloud backup (optional)
- â¬œ Advanced measurements

### Phase 3 (Q3 2026)
- â¬œ AI-assisted diagnosis
- â¬œ Integration with SIRENAGEST
- â¬œ Multi-hospital network

---

**Made with â¤ï¸ for Rural Health in Colombia**
EOF

    log_success "Main README.md created"
}

# ============================================================================
# BACKUP & SAFETY
# ============================================================================

create_backup() {
    local source_dir="$1"
    
    print_section "Creating Safety Backup"
    
    if [[ ! -d "$source_dir" ]]; then
        log_error "Source directory does not exist: $source_dir"
        return 1
    fi
    
    log_info "Backing up to: ${BACKUP_DIR}"
    
    if cp -r "$source_dir" "$BACKUP_DIR"; then
        log_success "Backup created successfully"
        log_info "Backup size: $(du -sh "$BACKUP_DIR" | cut -f1)"
        return 0
    else
        log_error "Backup failed!"
        return 1
    fi
}

# ============================================================================
# VERIFICATION & INTEGRITY
# ============================================================================

verify_migration() {
    local dest_dir="$1"
    
    print_section "Verifying Migration Integrity"
    
    local checks_passed=0
    local checks_total=5
    
    # Check 1: Production file exists
    if [[ -f "${dest_dir}/ECO-COL-PRODUCTION.html" ]]; then
        log_success "Production file exists"
        ((checks_passed++))
    else
        log_error "Production file missing!"
    fi
    
    # Check 2: All main directories created
    if [[ -d "${dest_dir}/1-BUSINESS-LOGIC" ]] && 
       [[ -d "${dest_dir}/5-DATA" ]] &&
       [[ -d "${dest_dir}/7-TESTING" ]]; then
        log_success "Core directories exist"
        ((checks_passed++))
    else
        log_error "Missing core directories!"
    fi
    
    # Check 3: Installers migrated
    local installer_count=$(find "${dest_dir}/9-TOOLS/installers" -name "*.sh" 2>/dev/null | wc -l)
    if [[ $installer_count -gt 0 ]]; then
        log_success "Installers migrated ($installer_count files)"
        ((checks_passed++))
    else
        log_warning "No installers found"
    fi
    
    # Check 4: Documentation present
    if [[ -f "${dest_dir}/README.md" ]]; then
        log_success "Main README exists"
        ((checks_passed++))
    else
        log_error "Main README missing!"
    fi
    
    # Check 5: Archive has content
    local archive_count=$(find "${dest_dir}/ARCHIVE" -type f 2>/dev/null | wc -l)
    if [[ $archive_count -gt 0 ]]; then
        log_success "Archive contains ${archive_count} files"
        ((checks_passed++))
    else
        log_warning "Archive is empty"
    fi
    
    echo ""
    log_info "Verification: ${checks_passed}/${checks_total} checks passed"
    
    if [[ $checks_passed -eq $checks_total ]]; then
        return 0
    else
        return 1
    fi
}

# ============================================================================
# FINAL STATISTICS & SUMMARY
# ============================================================================

print_summary() {
    print_header "REORGANIZATION COMPLETE"
    
    echo -e "${GREEN}âœ“ Successfully reorganized ECO-COL project${NC}"
    echo ""
    echo -e "${CYAN}Statistics:${NC}"
    echo -e "  Files processed: ${WHITE}${TOTAL_FILES}${NC}"
    echo -e "  Files migrated:  ${GREEN}${MIGRATED_FILES}${NC}"
    echo -e "  Files archived:  ${YELLOW}${ARCHIVED_FILES}${NC}"
    echo -e "  Errors:          ${RED}${ERRORS}${NC}"
    echo ""
    echo -e "${CYAN}Key Outputs:${NC}"
    echo -e "  Production file: ${GREEN}ECO-COL-FINAL/ECO-COL-PRODUCTION.html${NC}"
    echo -e "  Documentation:   ${BLUE}ECO-COL-FINAL/README.md${NC}"
    echo -e "  Migration log:   ${BLUE}${LOG_FILE}${NC}"
    echo -e "  Backup location: ${YELLOW}${BACKUP_DIR}${NC}"
    echo ""
    echo -e "${CYAN}Next Steps:${NC}"
    echo -e "  1. Review migration report in ECO-COL-FINAL/"
    echo -e "  2. Test production file in browser"
    echo -e "  3. Verify all installers in 9-TOOLS/"
    echo -e "  4. Read 8-DOCS/ for deployment guide"
    echo ""
    
    if [[ $ERRORS -eq 0 ]]; then
        echo -e "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
        echo -e "${GREEN}â•‘  MIGRATION SUCCESSFUL - NO ERRORS!   â•‘${NC}"
        echo -e "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    else
        echo -e "${RED}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
        echo -e "${RED}â•‘  MIGRATION COMPLETED WITH ERRORS      â•‘${NC}"
        echo -e "${RED}â•‘  Review ${LOG_FILE}  â•‘${NC}"
        echo -e "${RED}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    fi
    echo ""
}

# ============================================================================
# MAIN EXECUTION
# ============================================================================

main() {
    print_header "ECO-COL PROFESSIONAL REORGANIZER v${SCRIPT_VERSION}"
    
    # Prompt for source directory
    echo -e "${CYAN}Enter source directory path (e.g., /mnt/user-data/uploads):${NC}"
    read -r SOURCE_DIR
    
    # Validate source directory
    if [[ ! -d "$SOURCE_DIR" ]]; then
        log_error "Source directory does not exist: ${SOURCE_DIR}"
        exit 1
    fi
    
    local DEST_DIR="/home/claude/ECO-COL-FINAL"
    
    echo ""
    log_info "Source: ${SOURCE_DIR}"
    log_info "Destination: ${DEST_DIR}"
    log_info "Backup: ${BACKUP_DIR}"
    echo ""
    
    read -p "Proceed with reorganization? (yes/no): " -r
    if [[ ! $REPLY =~ ^[Yy]es$ ]]; then
        log_warning "Operation cancelled by user"
        exit 0
    fi
    
    # Execute reorganization pipeline
    create_backup "$SOURCE_DIR" || exit 1
    
    create_directory_structure "$DEST_DIR"
    
    perform_migration "$SOURCE_DIR" "$DEST_DIR"
    
    generate_migration_report "$DEST_DIR"
    
    generate_main_readme "$DEST_DIR"
    
    verify_migration "$DEST_DIR"
    
    print_summary
    
    log_info "Full log available at: ${LOG_FILE}"
}

# Entry point
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
